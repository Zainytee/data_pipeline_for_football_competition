# âš½ Data Pipeline For Football Competition

Football competitions â€” leagues, cups, and tournaments â€” fuel the global football economy.  
Behind every kickoff lies an ecosystem of **data flows**: competitions are scheduled, tracked, and analyzed across systems.  

This project brings that process to life by building a **scalable data pipeline**:  

- **Extracts competition data** from a football API  
- **Loads** it into **Postgres RDS** (via Dockerized Airflow)  
- **Replicates** it into **Amazon Redshift** (via Airbyte on Minikube)  
- **Automates & orchestrates** the entire journey with Airflow and Airbyte  


---

## ğŸ—ï¸ Architecture  

```text
API (Football Competitions) 
       â”‚
       â–¼
Airflow DAG (Dockerized, Python)
       â”‚
       â–¼
Postgres RDS (Staging)
       â”‚
   Airbyte (Minikube)
       â”‚
       â–¼
Amazon Redshift (Analytics)
```

- **Airflow (Dockerized)** â†’ Extracts data from API â†’ loads into RDS  
- **Airbyte** â†’ Replicates data from RDS â†’ Redshift  
- **Minikube** â†’ Local Kubernetes cluster hosting Airbyte pods  
- **Redshift** â†’ Analytics-ready warehouse  

ğŸ“¸ **Architecture Diagram**  
![Architecture](./images/architecture.png)  

---

## ğŸ”„ Data Flow  

1. **Airflow DAG** (running in Docker) extracts API data and writes it into RDS.  
2. **Airbyte connectors** replicate RDS â†’ Redshift.  
3. **Redshift queries** (e.g., `SELECT * FROM competitions;`) confirm successful ingestion.  

---

## ğŸ“¸ Pipeline Screenshots  

- **Airflow DAG**  
  ![Airflow DAG](./images/dag_competition.png)  

- **Redshift Query (`SELECT * FROM competition`)**  
  ![Redshift Query](./images/query_result.png)  

- **Minikube Pods (Airbyte on Kubernetes)**  
  ![Minikube Pods](./images/minikube_airbytes_pods.png)  

- **Airbyte Sync UI**  
  ![Airbyte UI](./images/airbyte_connection_UI.png)  

---

## ğŸš€ Quick Start  

### 1. Clone Repository  
```bash
git clone https://github.com/YOUR_USERNAME/football_competition_pipeline.git
cd football_competition_pipeline
```

### 2. Set Environment Variables  
Configure API and DB credentials:  
```bash
export API_KEY=your_api_key_here
export RDS_HOST=your_rds_host
export RDS_USER=your_rds_user
export RDS_PASSWORD=your_rds_password
export RDS_DB=football_competition
```

### 3. Run Airflow (Dockerized)  
```bash
# Start Airflow containers
docker-compose up -d

# Access Airflow UI
http://localhost:8080
```

Trigger your DAG:  
```bash
airflow dags trigger football_competition_dag
```

### 4. Start Airbyte on Minikube  
```bash
minikube start

kubectl get pods -n airbyte
```
Then configure the `RDS â†’ Redshift` connection in Airbyteâ€™s UI.  

### 5. Query in Redshift  
```sql
SELECT * FROM competitions LIMIT 10;
```

---

## ğŸ› ï¸ Tech Stack  

- **Python** â†’ API extraction scripts  
- **Apache Airflow (Dockerized)** â†’ Containerized orchestration (API â†’ RDS)  
- **Postgres RDS** â†’ Staging database  
- **Amazon Redshift** â†’ Analytics warehouse  
- **Airbyte (Minikube)** â†’ ELT pipeline (RDS â†’ Redshift)  
- **Terraform** â†’ (Optional) Infrastructure provisioning  

---

## ğŸ“Š Use Cases  

With competition data in Redshift, analysts can:  
- Track **active competitions** across seasons  
- Compare **league growth across regions**  
- Power dashboards for **tournament analytics**  

---

## âœ¨ Closing Note  

This pipeline reflects how **real-world data platforms** integrate APIs, relational databases, ELT tools, and cloud warehouses.  
It is **guided by the principles of scalability, automation, and building scalable data platforms**, while telling the story of football competitions around the globe.  
